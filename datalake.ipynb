{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMi4DjizwmAdjsePDwBnRa6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tannyyaawadhwani-design/puthony/blob/main/datalake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHW_3WKH-JyR"
      },
      "outputs": [],
      "source": [
        "!pip install -q boto3 pandas matplotlib\n",
        "#Importing libraries\n",
        "import os, time, boto3, botocore\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from io import StringIO\n",
        "\n",
        "#Upload your access-keys CSV (choose tannyyaacollab_accesskeys.csv)\n",
        "print(\"Please upload your access-keys CSV (tannyyaacollab_accesskeys.csv). Use the file chooser that appears.\")\n",
        "uploaded = files.upload()  # interactively choose the CSV\n",
        "\n",
        "#Read keys from uploaded CSV (try common column names)\n",
        "key_file = None\n",
        "for name in uploaded.keys():\n",
        "    if name.lower().endswith(\".csv\"):\n",
        "        key_file = name\n",
        "        break\n",
        "if not key_file:\n",
        "    raise SystemExit(\"No CSV uploaded. Please upload tannyyaacollab_accesskeys.csv and re-run this cell.\")\n",
        "\n",
        "df_keys = pd.read_csv(key_file)\n",
        "# Attempt to find columns - common IAM CSV columns:\n",
        "possible_access_cols = [\"Access key ID\", \"Access key id\", \"Access Key ID\", \"AccessKeyId\", \"access_key_id\", \"AccessKeyId\"]\n",
        "possible_secret_cols = [\"Secret access key\", \"Secret Access Key\", \"SecretAccessKey\", \"SecretKey\", \"secret_access_key\"]\n",
        "\n",
        "def find_col(df, candidates):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    # try lowercase matching\n",
        "    lc = {col.lower(): col for col in df.columns}\n",
        "    for c in candidates:\n",
        "        if c.lower() in lc:\n",
        "            return lc[c.lower()]\n",
        "    return None\n",
        "\n",
        "access_col = find_col(df_keys, possible_access_cols)\n",
        "secret_col = find_col(df_keys, possible_secret_cols)\n",
        "if access_col is None or secret_col is None:\n",
        "    print(\"Couldn't autodetect Access Key or Secret Key columns. Here are the CSV columns:\", list(df_keys.columns))\n",
        "    raise SystemExit(\"Rename columns or provide a CSV with 'Access key ID' and 'Secret access key' columns.\")\n",
        "\n",
        "access_key = str(df_keys.iloc[0][access_col]).strip()\n",
        "secret_key = str(df_keys.iloc[0][secret_col]).strip()\n",
        "\n",
        "#Set environment vars for this Colab session\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = access_key\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = secret_key\n",
        "\n",
        "# Default region (change if you want)\n",
        "DEFAULT_REGION = \"eu-north-1\"   # change if needed\n",
        "os.environ.setdefault(\"AWS_DEFAULT_REGION\", DEFAULT_REGION)\n",
        "os.environ.setdefault(\"DATA_LAKE_BUCKET\", \"tanya-datalake-demo-2025\")\n",
        "\n",
        "REGION = os.getenv(\"AWS_DEFAULT_REGION\")\n",
        "BUCKET = os.getenv(\"DATA_LAKE_BUCKET\")\n",
        "ATHENA_DB = os.getenv(\"ATHENA_DB\", \"demo_db\")\n",
        "ATHENA_TABLE = os.getenv(\"ATHENA_TABLE\", \"orders\")\n",
        "ATHENA_OUTPUT = f\"s3://{BUCKET}/athena-results/\"\n",
        "\n",
        "print(\"Using AWS region:\", REGION)\n",
        "print(\"Using S3 bucket:\", BUCKET)\n",
        "\n",
        "#Creating local sample CSV\n",
        "sample_csv = \"\"\"order_id,customer_id,amount,order_date\n",
        "1,tanya_001,100.5,2025-09-01\n",
        "2,shivani_002,50.0,2025-09-01\n",
        "3,rishi_003,75.25,2025-09-02\n",
        "4,shubham_004,123.00,2025-09-03\n",
        "5,tanushree_005,10.00,2025-09-03\n",
        "6,priyanka_006,500.00,2025-09-04\n",
        "7,ankita_007,9.99,2025-09-04\n",
        "8,khushi_008,250.00,2025-09-05\n",
        "\"\"\"\n",
        "LOCAL_FILE = \"sample_orders.csv\"\n",
        "with open(LOCAL_FILE, \"w\") as f:\n",
        "    f.write(sample_csv)\n",
        "print(\"Created sample CSV:\", LOCAL_FILE)\n",
        "\n",
        "#Init boto3 clients\n",
        "s3 = boto3.client(\"s3\", region_name=REGION)\n",
        "athena = boto3.client(\"athena\", region_name=REGION)\n",
        "\n",
        "#Ensure bucket exists and upload the CSV\n",
        "def ensure_bucket_and_upload(bucket, local_file, s3_key):\n",
        "    try:\n",
        "        s3.head_bucket(Bucket=bucket)\n",
        "        print(\"Bucket exists:\", bucket)\n",
        "    except botocore.exceptions.ClientError as e:\n",
        "        err = e.response.get(\"Error\", {})\n",
        "        code = err.get(\"Code\", \"\")\n",
        "        # create bucket if not present\n",
        "        print(\"Bucket not found or not accessible. Attempting to create bucket:\", bucket)\n",
        "        s3.create_bucket(Bucket=bucket, CreateBucketConfiguration={\"LocationConstraint\": REGION})\n",
        "        s3.put_bucket_versioning(Bucket=bucket, VersioningConfiguration={\"Status\": \"Enabled\"})\n",
        "        print(\"Created bucket:\", bucket)\n",
        "    s3.upload_file(local_file, bucket, s3_key)\n",
        "    print(f\"Uploaded {local_file} to s3://{bucket}/{s3_key}\")\n",
        "\n",
        "S3_KEY = \"raw/orders/sample_orders.csv\"\n",
        "ensure_bucket_and_upload(BUCKET, LOCAL_FILE, S3_KEY)\n",
        "\n",
        "#Helper to run Athena SQL and wait\n",
        "def run_athena_sql(sql, database=ATHENA_DB, output=ATHENA_OUTPUT, wait=True, poll_seconds=1):\n",
        "    resp = athena.start_query_execution(\n",
        "        QueryString=sql,\n",
        "        QueryExecutionContext={\"Database\": database},\n",
        "        ResultConfiguration={\"OutputLocation\": output}\n",
        "    )\n",
        "    qid = resp[\"QueryExecutionId\"]\n",
        "    if not wait:\n",
        "        return qid\n",
        "    while True:\n",
        "        status = athena.get_query_execution(QueryExecutionId=qid)[\"QueryExecution\"][\"Status\"][\"State\"]\n",
        "        if status in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n",
        "            break\n",
        "        time.sleep(poll_seconds)\n",
        "    return status, qid\n",
        "\n",
        "#Creating Athena database and external table (CSV)\n",
        "print(\"Creating Athena database and table...\")\n",
        "status, qid = run_athena_sql(f\"CREATE DATABASE IF NOT EXISTS {ATHENA_DB};\")\n",
        "print(\"Create DB status:\", status)\n",
        "\n",
        "ddl = f\"\"\"\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.{ATHENA_TABLE} (\n",
        "  order_id INT,\n",
        "  customer_id STRING,\n",
        "  amount DOUBLE,\n",
        "  order_date STRING\n",
        ")\n",
        "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
        "WITH SERDEPROPERTIES (\n",
        "  \"separatorChar\" = \",\",\n",
        "  \"quoteChar\"     = '\"'\n",
        ")\n",
        "LOCATION 's3://{BUCKET}/raw/orders/'\n",
        "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
        "\"\"\"\n",
        "status, qid = run_athena_sql(ddl)\n",
        "print(\"Create table status:\", status)\n",
        "\n",
        "#Query Athena and return pandas DataFrame\n",
        "def query_athena_to_df(sql, database=ATHENA_DB, output=ATHENA_OUTPUT):\n",
        "    print(\"Running query:\", sql)\n",
        "    resp = athena.start_query_execution(\n",
        "        QueryString=sql,\n",
        "        QueryExecutionContext={\"Database\": database},\n",
        "        ResultConfiguration={\"OutputLocation\": output}\n",
        "    )\n",
        "    qid = resp[\"QueryExecutionId\"]\n",
        "    # wait\n",
        "    while True:\n",
        "        r = athena.get_query_execution(QueryExecutionId=qid)\n",
        "        st = r[\"QueryExecution\"][\"Status\"][\"State\"]\n",
        "        if st in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n",
        "            break\n",
        "        time.sleep(1)\n",
        "    if st != \"SUCCEEDED\":\n",
        "        reason = r[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"\")\n",
        "        raise RuntimeError(f\"Athena query {qid} failed: {st}. {reason}\")\n",
        "\n",
        "    # get results\n",
        "    result = athena.get_query_results(QueryExecutionId=qid)\n",
        "    cols = [c[\"Name\"] for c in result[\"ResultSet\"][\"ResultSetMetadata\"][\"ColumnInfo\"]]\n",
        "    rows = []\n",
        "    for row in result[\"ResultSet\"][\"Rows\"][1:]:\n",
        "        rows.append([d.get(\"VarCharValue\") for d in row[\"Data\"]])\n",
        "    df = pd.DataFrame(rows, columns=cols)\n",
        "    return df\n",
        "\n",
        "#Read table into DataFrame\n",
        "df = query_athena_to_df(f\"SELECT * FROM {ATHENA_TABLE} LIMIT 100;\")\n",
        "print(\"Rows returned:\", len(df))\n",
        "display(df)\n",
        "\n",
        "#Converting types and save\n",
        "if \"amount\" in df.columns:\n",
        "    df[\"amount\"] = df[\"amount\"].astype(float)\n",
        "\n",
        "OUT_LOCAL = \"athena_query_result.csv\"\n",
        "df.to_csv(OUT_LOCAL, index=False)\n",
        "print(\"Saved query result to local file:\", OUT_LOCAL)\n",
        "\n",
        "#Uploading processed CSV to S3 processed zone\n",
        "processed_key = \"processed/orders/athena_query_result.csv\"\n",
        "s3.upload_file(OUT_LOCAL, BUCKET, processed_key)\n",
        "print(f\"Uploaded processed CSV to s3://{BUCKET}/{processed_key}\")\n",
        "\n",
        "#bar chart\n",
        "if \"customer_id\" in df.columns and \"amount\" in df.columns:\n",
        "    plt.figure(figsize=(9,4))\n",
        "    df.plot(kind=\"bar\", x=\"customer_id\", y=\"amount\", legend=False)\n",
        "    plt.title(\"Order Amounts per Customer\")\n",
        "    plt.ylabel(\"amount\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"All done. Remember to rotate or delete access keys after testing if these are long-lived keys.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}